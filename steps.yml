# Robot Framework Ansible Testing Project Setup Guide

## Prerequisites

Before starting, ensure you have the following installed on your MacBook:

### 1. Install Python and pip
```bash
# Check if Python is installed
python3 --version

# If not installed, install via Homebrew
brew install python3
```

### 2. Install VS Code
Download and install from: https://code.visualstudio.com/

### 3. Install VS Code Extensions
- Robot Framework Language Server
- Python
- YAML

## Project Setup

### 1. Create Project Directory
```bash
mkdir ansible-robotframework-tests
cd ansible-robotframework-tests
```

### 2. Create Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Required Dependencies
```bash
pip install robotframework
pip install robotframework-sshlibrary
pip install robotframework-requests
pip install pyyaml
pip install ansible
```

### 4. Create Project Structure
```bash
mkdir -p tests/ansible
mkdir -p resources
mkdir -p libraries
mkdir -p results
mkdir -p config
mkdir -p playbooks
```

## Project Files

### 1. requirements.txt
Create `requirements.txt`:
```
robotframework==6.1.1
robotframework-sshlibrary==3.8.0
robotframework-requests==0.9.4
PyYAML==6.0.1
ansible==8.5.0
```

### 2. Custom Ansible Library
Create `libraries/AnsibleLibrary.py`:
```python
import subprocess
import yaml
import json
import os
from robot.api.deco import keyword
from robot.api import logger

class AnsibleLibrary:
    """Custom Robot Framework library for Ansible operations"""
    
    def __init__(self):
        self.playbook_results = {}
        self.current_playbook = None
    
    @keyword
    def run_ansible_playbook(self, playbook_path, inventory_path=None, extra_vars=None):
        """Execute an Ansible playbook and capture results"""
        cmd = ['ansible-playbook', playbook_path]
        
        if inventory_path:
            cmd.extend(['-i', inventory_path])
        
        if extra_vars:
            cmd.extend(['--extra-vars', extra_vars])
        
        # Add verbose output for detailed logs
        cmd.append('-v')
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=os.getcwd()
            )
            
            self.playbook_results[playbook_path] = {
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'command': ' '.join(cmd)
            }
            
            self.current_playbook = playbook_path
            logger.info(f"Playbook execution completed with return code: {result.returncode}")
            
            return result.returncode == 0
            
        except Exception as e:
            logger.error(f"Failed to execute playbook: {str(e)}")
            raise
    
    @keyword
    def get_playbook_status(self, playbook_path=None):
        """Get the execution status of a playbook"""
        playbook = playbook_path or self.current_playbook
        
        if playbook not in self.playbook_results:
            raise ValueError(f"No results found for playbook: {playbook}")
        
        result = self.playbook_results[playbook]
        return "success" if result['return_code'] == 0 else "failed"
    
    @keyword
    def verify_no_failures(self, playbook_path=None):
        """Verify that the playbook completed without failures"""
        playbook = playbook_path or self.current_playbook
        
        if playbook not in self.playbook_results:
            raise ValueError(f"No results found for playbook: {playbook}")
        
        result = self.playbook_results[playbook]
        output = result['stdout'].lower()
        
        # Check for failure indicators
        failure_indicators = ['failed=', 'unreachable=', 'fatal:']
        
        for indicator in failure_indicators:
            if indicator in output and not output.split(indicator)[1].split()[0].startswith('0'):
                return False
        
        return result['return_code'] == 0
    
    @keyword
    def check_for_warnings_and_skipped(self, playbook_path=None):
        """Check playbook output for warnings or skipped tasks"""
        playbook = playbook_path or self.current_playbook
        
        if playbook not in self.playbook_results:
            raise ValueError(f"No results found for playbook: {playbook}")
        
        result = self.playbook_results[playbook]
        output = result['stdout']
        
        warnings = []
        skipped_tasks = []
        
        for line in output.split('\n'):
            if '[WARNING]' in line or 'WARN' in line:
                warnings.append(line.strip())
            elif 'skipping:' in line or 'SKIPPED' in line:
                skipped_tasks.append(line.strip())
        
        return {
            'warnings': warnings,
            'skipped_tasks': skipped_tasks,
            'warning_count': len(warnings),
            'skipped_count': len(skipped_tasks)
        }
    
    @keyword
    def get_git_commit_hash(self, repository_path='.'):
        """Get the current git commit hash"""
        try:
            result = subprocess.run(
                ['git', 'rev-parse', 'HEAD'],
                capture_output=True,
                text=True,
                cwd=repository_path
            )
            return result.stdout.strip() if result.returncode == 0 else "unknown"
        except Exception:
            return "unknown"
    
    @keyword
    def validate_playbook_variables(self, playbook_path, expected_vars=None):
        """Validate that playbook variables are properly substituted"""
        try:
            with open(playbook_path, 'r') as file:
                content = file.read()
            
            # Check for unsubstituted variables (basic check)
            unsubstituted = []
            import re
            
            # Look for {{ variable_name }} patterns that might be unsubstituted
            pattern = r'\{\{\s*([^}]+)\s*\}\}'
            matches = re.findall(pattern, content)
            
            for match in matches:
                # Skip Jinja2 functions and filters
                if not any(keyword in match for keyword in ['ansible_', 'hostvars', 'group_names', 'groups']):
                    unsubstituted.append(match.strip())
            
            result = {
                'unsubstituted_vars': unsubstituted,
                'substitution_complete': len(unsubstituted) == 0
            }
            
            return result
        
        except Exception as e:
            logger.error(f"Failed to validate variables: {str(e)}")
            raise
    
    @keyword
    def log_playbook_version(self, playbook_path=None):
        """Log the playbook version and commit information"""
        playbook = playbook_path or self.current_playbook
        commit_hash = self.get_git_commit_hash()
        
        version_info = {
            'playbook': playbook,
            'git_commit': commit_hash,
            'ansible_version': self._get_ansible_version()
        }
        
        logger.info(f"Playbook Version Info: {json.dumps(version_info, indent=2)}")
        return version_info
    
    def _get_ansible_version(self):
        """Get Ansible version"""
        try:
            result = subprocess.run(
                ['ansible', '--version'],
                capture_output=True,
                text=True
            )
            return result.stdout.split('\n')[0] if result.returncode == 0 else "unknown"
        except Exception:
            return "unknown"
```

### 3. Resource File
Create `resources/ansible_keywords.robot`:
```robot
*** Settings ***
Library    ../libraries/AnsibleLibrary.py
Library    OperatingSystem
Library    Collections

*** Variables ***
${PLAYBOOK_DIR}        ${CURDIR}/../playbooks
${INVENTORY_FILE}      ${CURDIR}/../config/inventory.yml
${RESULTS_DIR}         ${CURDIR}/../results

*** Keywords ***
Setup Ansible Test Environment
    [Documentation]    Setup test environment for Ansible testing
    Create Directory    ${RESULTS_DIR}
    Set Test Variable    ${TEST_START_TIME}    ${EMPTY}

Execute Playbook With Validation
    [Arguments]    ${playbook_name}    ${extra_vars}=${EMPTY}
    [Documentation]    Execute playbook and perform comprehensive validation
    
    ${playbook_path}=    Set Variable    ${PLAYBOOK_DIR}/${playbook_name}
    ${execution_success}=    Run Ansible Playbook    ${playbook_path}    ${INVENTORY_FILE}    ${extra_vars}
    
    Should Be True    ${execution_success}    Playbook execution failed
    
    # Store playbook name for other keywords
    Set Test Variable    ${CURRENT_PLAYBOOK}    ${playbook_path}
    
    RETURN    ${execution_success}

Validate Playbook Execution
    [Documentation]    Comprehensive validation of playbook execution
    
    # 1. Verify no failures
    ${no_failures}=    Verify No Failures
    Should Be True    ${no_failures}    Playbook completed with failures
    
    # 2. Check status
    ${status}=    Get Playbook Status
    Should Be Equal    ${status}    success    Playbook status is not success
    
    # 3. Check for warnings and skipped tasks
    ${warnings_and_skipped}=    Check For Warnings And Skipped
    Log    Warnings found: ${warnings_and_skipped['warning_count']}
    Log    Skipped tasks: ${warnings_and_skipped['skipped_count']}
    
    # 4. Log version information
    ${version_info}=    Log Playbook Version
    Log    Version Info: ${version_info}
    
    # 5. Validate variables
    ${var_validation}=    Validate Playbook Variables    ${CURRENT_PLAYBOOK}
    Should Be True    ${var_validation['substitution_complete']}    Variables not properly substituted
    
    RETURN    ${version_info}

Create Test Report
    [Arguments]    ${playbook_name}    ${version_info}
    [Documentation]    Create a summary test report
    
    ${report_content}=    Create List
    ...    Ansible Playbook Validation Report
    ...    =====================================
    ...    Playbook: ${playbook_name}
    ...    Status: success
    ...    No configuration drift: Verified
    ...    Git Commit: ${version_info['git_commit']}
    ...    Ansible Version: ${version_info['ansible_version']}
    ...    All variables validated: Yes
    ...    Test Date: ${EMPTY}
    
    ${report_file}=    Set Variable    ${RESULTS_DIR}/validation_report_${playbook_name}.txt
    Create File    ${report_file}    ${EMPTY}
    
    FOR    ${line}    IN    @{report_content}
        Append To File    ${report_file}    ${line}\n
    END
    
    Log    Test report created: ${report_file}
```

### 4. Main Test Suite
Create `tests/ansible/ansible_playbook_validation.robot`:
```robot
*** Settings ***
Documentation     Ansible Playbook Validation Test Suite
Resource          ../../resources/ansible_keywords.robot
Suite Setup       Setup Ansible Test Environment
Test Tags         ansible    validation    playbook

*** Variables ***
${PLAYBOOK_NAME}      site.yml
${TEST_EXTRA_VARS}    {"environment": "test", "debug_mode": true}

*** Test Cases ***
Validate Ansible Build Playbook Execution
    [Documentation]    Comprehensive validation of Ansible playbook execution
    [Tags]    critical    build
    
    Log    Starting Ansible playbook validation test
    
    # Execute the playbook
    ${success}=    Execute Playbook With Validation    ${PLAYBOOK_NAME}    ${TEST_EXTRA_VARS}
    
    # Perform comprehensive validation
    ${version_info}=    Validate Playbook Execution
    
    # Create test report
    Create Test Report    ${PLAYBOOK_NAME}    ${version_info}
    
    # Final assertions for test completion
    Should Be True    ${success}    Overall playbook validation failed
    
    Log    Ansible playbook validation completed successfully

Verify Playbook Status And Logs
    [Documentation]    Detailed verification of playbook status and execution logs
    [Tags]    verification    logs
    
    # This test assumes the previous test has run and set up the playbook results
    ${status}=    Get Playbook Status
    Should Be Equal    ${status}    success
    
    ${warnings_and_skipped}=    Check For Warnings And Skipped
    
    # Log detailed information
    Log Many
    ...    Playbook Status: ${status}
    ...    Warning Count: ${warnings_and_skipped['warning_count']}
    ...    Skipped Task Count: ${warnings_and_skipped['skipped_count']}
    
    # Verify acceptable thresholds (customize as needed)
    Should Be True    ${warnings_and_skipped['warning_count']} < 5    Too many warnings detected
    
Configuration Drift Detection
    [Documentation]    Validate no configuration drift exists
    [Tags]    drift    configuration
    
    # Note: This is a placeholder for actual drift detection logic
    # In practice, you would compare desired state vs actual state
    
    Log    Checking for configuration drift...
    
    # For demonstration, we'll validate that no failures occurred
    # which indicates the desired state was achieved
    ${no_failures}=    Verify No Failures
    Should Be True    ${no_failures}    Configuration drift detected - playbook failures found
    
    Log    No configuration drift detected

Version And Variable Validation
    [Documentation]    Validate playbook version tracking and variable substitution
    [Tags]    version    variables
    
    ${version_info}=    Log Playbook Version
    Should Not Be Equal    ${version_info['git_commit']}    unknown    Git commit hash not available
    
    ${var_validation}=    Validate Playbook Variables    ${PLAYBOOK_DIR}/${PLAYBOOK_NAME}
    Should Be True    ${var_validation['substitution_complete']}    Variable substitution incomplete
    
    Log Many
    ...    Git Commit: ${version_info['git_commit']}
    ...    Variable Substitution: Complete
    ...    Unsubstituted Variables: ${var_validation['unsubstituted_vars']}
```

### 5. Sample Configuration Files

Create `config/inventory.yml`:
```yaml
all:
  hosts:
    localhost:
      ansible_connection: local
  children:
    test_servers:
      hosts:
        localhost:
  vars:
    environment: test
    ansible_python_interpreter: "{{ ansible_playbook_python }}"
```

Create `playbooks/site.yml` (sample playbook):
```yaml
---
- name: Sample Ansible Playbook for Testing
  hosts: localhost
  gather_facts: yes
  vars:
    test_variable: "Hello World"
    environment: "{{ environment | default('development') }}"
    
  tasks:
    - name: Display test message
      debug:
        msg: "Testing Ansible playbook validation - {{ test_variable }}"
    
    - name: Create test directory
      file:
        path: "/tmp/ansible_test"
        state: directory
        mode: '0755'
    
    - name: Create test file
      copy:
        content: |
          Ansible Test File
          Environment: {{ environment }}
          Generated at: {{ ansible_date_time.iso8601 }}
        dest: "/tmp/ansible_test/test_file.txt"
        mode: '0644'
    
    - name: Verify test file exists
      stat:
        path: "/tmp/ansible_test/test_file.txt"
      register: test_file_stat
    
    - name: Assert test file was created
      assert:
        that:
          - test_file_stat.stat.exists
        fail_msg: "Test file was not created successfully"
        success_msg: "Test file created successfully"
```

## Running the Tests

### 1. Activate Virtual Environment
```bash
source venv/bin/activate
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run All Tests
```bash
robot --outputdir results tests/
```

### 4. Run Specific Test Suite
```bash
robot --outputdir results tests/ansible/ansible_playbook_validation.robot
```

### 5. Run Tests with Tags
```bash
# Run only critical tests
robot --outputdir results --include critical tests/

# Run only verification tests
robot --outputdir results --include verification tests/
```

### 6. Run Tests with Variables
```bash
robot --outputdir results --variable PLAYBOOK_NAME:custom.yml tests/
```

## VS Code Configuration

### 1. Create .vscode/settings.json
```json
{
    "python.defaultInterpreterPath": "./venv/bin/python",
    "robot.language-server.python": "./venv/bin/python",
    "robot.language-server.args": [
        "--log-level=DEBUG"
    ],
    "files.associations": {
        "*.robot": "robot"
    }
}
```

### 2. Create .vscode/launch.json for debugging
```json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Robot Framework",
            "type": "python",
            "request": "launch",
            "module": "robot",
            "args": [
                "--outputdir", "results",
                "tests/"
            ],
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}"
        }
    ]
}
```

## Project Structure Overview

```
ansible-robotframework-tests/
├── venv/                     # Virtual environment
├── tests/
│   └── ansible/
│       └── ansible_playbook_validation.robot
├── resources/
│   └── ansible_keywords.robot
├── libraries/
│   └── AnsibleLibrary.py
├── config/
│   └── inventory.yml
├── playbooks/
│   └── site.yml
├── results/                  # Test results and reports
├── .vscode/
│   ├── settings.json
│   └── launch.json
└── requirements.txt
```

## Customization Tips

1. **Modify the playbook path** in variables to point to your actual playbooks
2. **Update inventory files** to match your infrastructure
3. **Customize validation thresholds** (warning counts, etc.) in test cases
4. **Add more specific drift detection logic** based on your requirements
5. **Extend the AnsibleLibrary** with additional validation methods
6. **Configure CI/CD integration** by adding appropriate exit codes and reporting

## Viewing Results

After running tests, check the `results/` directory for:
- `report.html` - Detailed test execution report
- `log.html` - Comprehensive test logs
- `output.xml` - Machine-readable test results
- `validation_report_*.txt` - Custom validation reports

This setup provides a comprehensive Robot Framework testing solution for Ansible playbook validation with all the requirements you specified.
